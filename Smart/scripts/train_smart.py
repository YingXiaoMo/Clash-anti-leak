import os
import pandas as pd
import numpy as np
import lightgbm as lgb
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, RobustScaler
from go_parser import GoTransformParser
from typing import Tuple, List, Optional

# ===================================================================
# 1. é…ç½®ä¸­å¿ƒ
# ===================================================================
BASE_DIR = os.path.dirname(os.path.abspath(__file__))

DATA_FILE = os.path.join(BASE_DIR, "../data/smart_weight_data.csv")
CLEAN_DATA_FILE = os.path.join(BASE_DIR, "../data/smart_weight_data_clean.csv")
GO_FILE = os.path.join(BASE_DIR, "../go_transform/transform.go")
MODEL_FILE = os.path.join(BASE_DIR, "../models/Model.bin")

STD_SCALER_FEATURES = [
    'connect_time', 'latency', 'upload_mb', 'download_mb', 'duration_minutes',
    'last_used_seconds', 'traffic_density'
]
ROBUST_SCALER_FEATURES = ['success', 'failure']

LGBM_PARAMS = {
    'objective': 'regression', 'metric': 'rmse', 'n_estimators': 1000,
    'learning_rate': 0.03, 'random_state': 42, 'n_jobs': -1, 'device': 'gpu'
}
EARLY_STOPPING_ROUNDS = 100
EXPECTED_COLS = 37  # æ•°æ®åˆ—æ•°ï¼Œå¿…é¡»å’Œ CSV å®é™…åˆ—æ•°ä¸€è‡´


# ===================================================================
# 2. æ•°æ®æ¸…æ´—
# ===================================================================
def clean_csv(input_file: str, output_file: str) -> None:
    print(f"--> æ¸…æ´— CSV æ–‡ä»¶: {input_file}")
    with open(input_file, "r", encoding="utf-8") as f_in, open(output_file, "w", encoding="utf-8") as f_out:
        for i, line in enumerate(f_in):
            if line.count(",") == EXPECTED_COLS - 1:  # åˆ—æ•° = é€—å·æ•° + 1
                f_out.write(line)
            else:
                print(f"    è·³è¿‡ç¬¬ {i + 1} è¡Œ (åˆ—æ•°å¼‚å¸¸)")
    print(f"    æ¸…æ´—å®Œæˆï¼Œæ–°æ–‡ä»¶: {output_file}")


# ===================================================================
# 3. æ•°æ®åŠ è½½ä¸ç‰¹å¾å¤„ç†
# ===================================================================
def load_and_clean_data(file_path: str) -> Optional[pd.DataFrame]:
    try:
        data = pd.read_csv(file_path)
        data.dropna(subset=['weight'], inplace=True)
        data = data[data['weight'] > 0].copy()
        print(f"    æ•°æ®åŠ è½½æˆåŠŸï¼Œå…± {len(data)} æ¡æœ‰æ•ˆè®°å½•ã€‚")
        return data
    except Exception as e:
        print(f"    æ•°æ®åŠ è½½å¤±è´¥: {e}")
        return None


def extract_features_from_preprocessed(data: pd.DataFrame, feature_order: List[str]) -> Optional[
    Tuple[pd.DataFrame, pd.Series]]:
    try:
        X = data[feature_order]
        y = data['weight']
        return X, y
    except KeyError as e:
        print(f"    ç¼ºå°‘å¿…è¦ç‰¹å¾åˆ—: {e}")
        return None, None


def apply_feature_transforms(X: pd.DataFrame) -> Tuple[pd.DataFrame, StandardScaler, RobustScaler]:
    X_scaled = X.copy()

    std_scaler = StandardScaler()
    X_scaled[STD_SCALER_FEATURES] = std_scaler.fit_transform(X_scaled[STD_SCALER_FEATURES])

    robust_scaler = RobustScaler()
    X_scaled[ROBUST_SCALER_FEATURES] = robust_scaler.fit_transform(X_scaled[ROBUST_SCALER_FEATURES])

    return X_scaled, std_scaler, robust_scaler


# ===================================================================
# 4. æ¨¡å‹è®­ç»ƒä¸ä¿å­˜
# ===================================================================
def train_model(X_train: pd.DataFrame, y_train: pd.Series, X_test: pd.DataFrame, y_test: pd.Series) -> lgb.Booster:
    train_data = lgb.Dataset(X_train, label=y_train)
    test_data = lgb.Dataset(X_test, label=y_test, reference=train_data)
    model = lgb.train(
        LGBM_PARAMS,
        train_data,
        valid_sets=[test_data],
        callbacks=[lgb.early_stopping(EARLY_STOPPING_ROUNDS, verbose=True)]
    )
    return model


def save_model_and_config(model: lgb.Booster, std_scaler: StandardScaler, robust_scaler: RobustScaler,
                          feature_order: List[str]):
    model.save_model(MODEL_FILE, num_iteration=model.best_iteration)
    print(f"    æ¨¡å‹å·²ä¿å­˜: {MODEL_FILE}")


# ===================================================================
# 5. ä¸»æ‰§è¡Œæµç¨‹
# ===================================================================
def main():
    print("--- Mihomo Smart æ¨¡å‹è®­ç»ƒå¼€å§‹ ---")

    # 1. æ¸…æ´— CSV
    clean_csv(DATA_FILE, CLEAN_DATA_FILE)

    # 2. åˆå§‹åŒ–ç‰¹å¾é¡ºåº
    parser = GoTransformParser(GO_FILE)
    feature_order = parser.get_feature_order()

    # 3. åŠ è½½æ•°æ®
    data = load_and_clean_data(CLEAN_DATA_FILE)
    if data is None:
        return

    # 4. æå–ç‰¹å¾å’Œç›®æ ‡
    X, y = extract_features_from_preprocessed(data, feature_order)
    if X is None:
        return

    # 5. ç‰¹å¾å˜æ¢
    X_scaled, std_scaler, robust_scaler = apply_feature_transforms(X)

    # 6. åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

    # 7. è®­ç»ƒæ¨¡å‹
    model = train_model(X_train, y_train, X_test, y_test)

    # 8. ä¿å­˜æ¨¡å‹
    save_model_and_config(model, std_scaler, robust_scaler, feature_order)

    print("\nğŸ‰ --- è®­ç»ƒå®Œæˆ --- ğŸ‰")


if __name__ == "__main__":
    main()
