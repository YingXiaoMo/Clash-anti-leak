# ==============================================================================
# Mihomo V3 æ™ºèƒ½æƒé‡æ¨¡å‹è®­ç»ƒ (å·²é›†æˆ V3 äºŒè¿›åˆ¶ç¼–ç )
# å‡ºå“ï¼šå®‰æ ¼è§†ç•Œ
# åŠŸèƒ½ï¼šåŸºäºå†å²æ•°æ®è®­ç»ƒ LightGBM å›å½’æ¨¡å‹ï¼Œå¹¶å°†æ¨¡å‹å’ŒScaleré…ç½®æ‰“åŒ…ä¸ºV3æ ¼å¼ã€‚
# ==============================================================================

import re
import pandas as pd
import numpy as np
import lightgbm as lgb
import os
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, RobustScaler
from typing import Tuple, List, Optional, Dict, Any

# âš¡ é‡ç‚¹ï¼šå¯¼å…¥ smart_store_creator åº“ï¼Œç”¨äºç”Ÿæˆ V3 äºŒè¿›åˆ¶æ–‡ä»¶
# å‡è®¾ PyPI åŒ…å 'smart-store-creator' å¯¹åº”çš„ Python æ¨¡å—åæ˜¯ 'smart_store_creator'
try:
    from smart_store_creator import SmartStoreCreator
    CREATOR_AVAILABLE = True
except ImportError:
    CREATOR_AVAILABLE = False
    print("FATAL: smart_store_creator library not found. V3 encoding will fail.")

# ==============================================================================
# 1. Go æºç è§£ææ¨¡å— (GoTransformParser)
# ==============================================================================

class GoTransformParser:
    """
    Go æºç è§£æå™¨
    
    è´Ÿè´£è§£æ Go è¯­è¨€æºæ–‡ä»¶ä¸­çš„ç‰¹å¾é¡ºåºå®šä¹‰ï¼Œæå– getDefaultFeatureOrder å‡½æ•°ä¸­
    çš„ç‰¹å¾æ˜ å°„å…³ç³»ã€‚
    """
    
    def __init__(self, go_file_path: str):
        """
        åˆå§‹åŒ–è§£æå™¨
        """
        try:
            with open(go_file_path, 'r', encoding='utf-8') as f:
                self.content = f.read()
            print(f"æˆåŠŸåŠ è½½ Go æºæ–‡ä»¶: {go_file_path}")
        except FileNotFoundError:
            # åœ¨ GitHub Actions ä¸­ï¼Œå¦‚æœ transform.go ä¸åœ¨ Smart/scripts/ ä¸­ï¼Œè¿™é‡Œä¼šå¤±è´¥
            raise FileNotFoundError(
                f"Go æºæ–‡ä»¶ '{go_file_path}' æ²¡æ‰¾åˆ°ã€‚è¯·ç¡®ä¿æ–‡ä»¶å­˜åœ¨äº Smart/scripts/ ç›®å½•ä¸­ã€‚"
            )
        
        self.feature_order = self._parse_feature_order()
    
    def _parse_feature_order(self) -> List[str]:
        """
        è§£æç‰¹å¾é¡ºåº
        """
        print("å¼€å§‹è§£æ getDefaultFeatureOrder å‡½æ•°...")
        
        function_pattern = r'func getDefaultFeatureOrder\(\) map\[int\]string \{\s*return map\[int\]string\{(.*?)\}\s*\}'
        match = re.search(function_pattern, self.content, re.DOTALL)
        
        if not match:
            print("è­¦å‘Š: æ²¡æ‰¾åˆ° getDefaultFeatureOrder å‡½æ•°ï¼Œä½¿ç”¨é¢„å®šä¹‰ç‰¹å¾é¡ºåº")
            return self._get_fallback_feature_order()
        
        function_body = match.group(1)
        feature_pairs = re.findall(r'(\d+):\s*"([^"]+)"', function_body)
        
        if not feature_pairs:
            print("è­¦å‘Š: å‡½æ•°ä½“ä¸­æ— æœ‰æ•ˆç‰¹å¾å®šä¹‰ï¼Œä½¿ç”¨é¢„å®šä¹‰ç‰¹å¾é¡ºåº")
            return self._get_fallback_feature_order()
        
        feature_dict = {int(index): name for index, name in feature_pairs}
        sorted_features = [feature_dict[i] for i in sorted(feature_dict.keys())]
        
        print(f"æˆåŠŸè§£æ {len(sorted_features)} ä¸ªç‰¹å¾çš„é¡ºåºå®šä¹‰")
        return sorted_features
    
    def get_feature_order(self) -> List[str]:
        """
        è·å–ç‰¹å¾é¡ºåºåˆ—è¡¨
        """
        return self.feature_order
    
    def _get_fallback_feature_order(self) -> List[str]:
        """
        é¢„å®šä¹‰ç‰¹å¾é¡ºåº (ä½œä¸ºä¼˜é›…é™çº§)
        """
        # å®Œæ•´çš„ç‰¹å¾åˆ—è¡¨ï¼Œç”¨äº Go æºç è§£æå¤±è´¥æ—¶çš„å¤‡é€‰
        return [
            'success', 'failure', 'connect_time', 'latency', 'upload_mb', 'download_mb', 
            'duration_minutes', 'last_used_seconds', 'is_udp', 'is_tcp', 'asn_feature', 
            'country_feature', 'address_feature', 'port_feature', 'traffic_ratio', 
            'traffic_density', 'connection_type_feature', 'asn_hash', 'host_hash', 
            'ip_hash', 'geoip_hash'
        ]

# ==============================================================================
# 2. ç³»ç»Ÿé…ç½®å‚æ•°
# ==============================================================================

# æ–‡ä»¶è·¯å¾„é…ç½® (å·²ä¿®æ­£ä¸º GitHub Actions ç¯å¢ƒçš„ç›¸å¯¹è·¯å¾„)
# è„šæœ¬åœ¨ Smart/scripts/ ä¸­è¿è¡Œ
DATA_FILE = '../data/smart_weight_data.csv'  # æ•°æ®ä½äºä»“åº“æ ¹ç›®å½•çš„ data æ–‡ä»¶å¤¹
GO_FILE = 'transform.go'                     # transform.go ä½äº Smart/scripts/ ç›®å½•
MODEL_FILE = '../../models/Model.bin'        # æ¨¡å‹è¾“å‡ºåˆ°ä»“åº“æ ¹ç›®å½•çš„ models æ–‡ä»¶å¤¹

# ç‰¹å¾é¢„å¤„ç†é…ç½®
STD_SCALER_FEATURES = [
    'connect_time', 'latency', 'upload_mb', 'download_mb', 'duration_minutes', 
    'last_used_seconds', 'traffic_density'
]
ROBUST_SCALER_FEATURES = ['success', 'failure']

# LightGBMæ¨¡å‹è¶…å‚æ•°é…ç½®
LGBM_PARAMS = {
    'objective': 'regression',
    'metric': 'rmse',
    'n_estimators': 1000,
    'learning_rate': 0.03,
    'random_state': 42,
    'n_jobs': -1 
}

EARLY_STOPPING_ROUNDS = 100

# ==============================================================================
# 3. æ ¸å¿ƒåŠŸèƒ½æ¨¡å—
# ==============================================================================

def load_and_clean_data(file_path: str) -> Optional[pd.DataFrame]:
    """
    æ•°æ®åŠ è½½ä¸é¢„å¤„ç†
    """
    print(f"å¼€å§‹åŠ è½½æ•°æ®æ–‡ä»¶: {file_path}")
    
    # æ£€æŸ¥æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡® (åœ¨ GitHub Actions ä¸­ï¼Œéœ€è¦ç¡®ä¿ç›¸å¯¹è·¯å¾„æ­£ç¡®)
    absolute_data_path = os.path.abspath(file_path)
    if not os.path.exists(absolute_data_path):
        # è¿™é‡Œçš„ os.getcwd() åº”è¯¥æ˜¯ Smart/scripts/
        print(f"é”™è¯¯: æ•°æ®æ–‡ä»¶æœªæ‰¾åˆ°ï¼Œé¢„æœŸè·¯å¾„: {absolute_data_path}")
        return None

    try:
        data = pd.read_csv(absolute_data_path, on_bad_lines='skip')
        print(f"æ•°æ®åŠ è½½å®Œæˆï¼ŒåŸå§‹è®°å½•æ•°: {len(data)}")
    except Exception as e:
        print(f"æ•°æ®åŠ è½½å¤±è´¥: {e}")
        return None

    original_count = len(data)
    data.dropna(subset=['weight'], inplace=True)
    data = data[data['weight'] > 0].copy()
    final_count = len(data)
    filtered_count = original_count - final_count
    
    print(f"æ•°æ®æ¸…æ´—å®Œæˆ: {original_count} â†’ {final_count} æ¡è®°å½• (è¿‡æ»¤ {filtered_count} æ¡)")
    return data

def extract_features_from_preprocessed(data: pd.DataFrame, feature_order: List[str]) -> Optional[Tuple[pd.DataFrame, pd.Series]]:
    """
    ç‰¹å¾çŸ©é˜µæ„å»º
    """
    print("å¼€å§‹æ„å»ºç‰¹å¾çŸ©é˜µå’Œç›®æ ‡å˜é‡...")
    
    try:
        X = data[feature_order]
        y = data['weight']
        print(f"ç‰¹å¾æå–å®Œæˆ - ç‰¹å¾çŸ©é˜µå½¢çŠ¶: {X.shape}, ç›®æ ‡å˜é‡å½¢çŠ¶: {y.shape}")
        return X, y
        
    except KeyError as e:
        print(f"ç‰¹å¾æå–å¤±è´¥: ç¼ºå°‘å¿…è¦çš„ç‰¹å¾åˆ— {e}")
        return None, None

def apply_feature_transforms(X: pd.DataFrame, feature_order: List[str]) -> Tuple[pd.DataFrame, StandardScaler, RobustScaler]:
    """
    ç‰¹å¾æ ‡å‡†åŒ–å¤„ç†
    """
    print("å¼€å§‹ç‰¹å¾æ ‡å‡†åŒ–å¤„ç†...")
    X_transformed = X.copy()
    
    # 1. StandardScaler
    std_scaler = StandardScaler()
    std_features_available = [f for f in STD_SCALER_FEATURES if f in X_transformed.columns]
    
    if std_features_available:
        # åªå¯¹æ•°æ®æ¡†ä¸­å®é™…å­˜åœ¨çš„åˆ—è¿›è¡Œ fit_transform
        X_transformed[std_features_available] = std_scaler.fit_transform(X_transformed[std_features_available])
        print(f"StandardScaler å¤„ç†å®Œæˆï¼Œå½±å“ç‰¹å¾æ•°: {len(std_features_available)}")
    
    # 2. RobustScaler
    robust_scaler = RobustScaler()
    robust_features_available = [f for f in ROBUST_SCALER_FEATURES if f in X_transformed.columns]
    
    if robust_features_available:
        # åªå¯¹æ•°æ®æ¡†ä¸­å®é™…å­˜åœ¨çš„åˆ—è¿›è¡Œ fit_transform
        X_transformed[robust_features_available] = robust_scaler.fit_transform(X_transformed[robust_features_available])
        print(f"RobustScaler å¤„ç†å®Œæˆï¼Œå½±å“ç‰¹å¾æ•°: {len(robust_features_available)}")
    
    return X_transformed, std_scaler, robust_scaler

def train_lgbm_model(X_train: pd.DataFrame, y_train: pd.Series, X_test: pd.DataFrame, y_test: pd.Series) -> lgb.Booster:
    """
    LightGBM æ¨¡å‹è®­ç»ƒï¼Œè¿”å›åŸç”Ÿ Booster å¯¹è±¡
    """
    print("å¼€å§‹ LightGBM æ¨¡å‹è®­ç»ƒ...")
    
    # æ¨¡å‹çš„è®­ç»ƒéƒ¨åˆ†åº”è¯¥ä½¿ç”¨ lgb.train è€Œä¸æ˜¯ LGBMRegressor 
    # å› ä¸º lgb.train è¿”å›åŸç”Ÿ Booster å¯¹è±¡ï¼Œæ–¹ä¾¿åç»­ä¿å­˜å’Œ V3 ç¼–ç 
    train_data = lgb.Dataset(X_train, label=y_train)
    test_data = lgb.Dataset(X_test, label=y_test)

    model = lgb.train(
        LGBM_PARAMS,
        train_data,
        valid_sets=[test_data],
        callbacks=[lgb.early_stopping(EARLY_STOPPING_ROUNDS, verbose=False)]
    )
    
    # ä½¿ç”¨ Booster å¯¹è±¡è¯„ä¼° R2 å¾—åˆ†
    from sklearn.metrics import r2_score
    train_pred = model.predict(X_train, num_iteration=model.best_iteration)
    test_pred = model.predict(X_test, num_iteration=model.best_iteration)
    
    train_r2 = r2_score(y_train, train_pred)
    test_r2 = r2_score(y_test, test_pred)
    
    print(f"æ¨¡å‹è®­ç»ƒå®Œæˆã€‚æœ€ä½³è¿­ä»£æ¬¡æ•°: {model.best_iteration}")
    print(f"è®­ç»ƒé›†RÂ²å¾—åˆ†: {train_r2:.4f}")
    print(f"æµ‹è¯•é›†RÂ²å¾—åˆ†: {test_r2:.4f}")
    
    if test_r2 > 0.8:
        print("æ¨¡å‹æ€§èƒ½è¯„ä¼°: ä¼˜ç§€")
    else:
        print("æ¨¡å‹æ€§èƒ½è¯„ä¼°: è‰¯å¥½æˆ–éœ€è¦æ”¹è¿›")
    
    return model

def save_model_and_config(model: lgb.Booster, std_scaler: StandardScaler, robust_scaler: RobustScaler, feature_order: List[str], model_file: str) -> None:
    """
    æ¨¡å‹åºåˆ—åŒ–ä¿å­˜ (ä½¿ç”¨ V3 äºŒè¿›åˆ¶ç¼–ç )
    
    å°† LightGBM æ¨¡å‹å’Œ Scaler é…ç½®ä¸€èµ·æ‰“åŒ…ä¸º Mihomo V3 è¦æ±‚çš„äºŒè¿›åˆ¶æ ¼å¼ã€‚
    """
    print("--> ç¼–ç  LightGBM æ¨¡å‹åˆ° Mihomo V3 äºŒè¿›åˆ¶æ ¼å¼...")

    if not CREATOR_AVAILABLE:
        raise Exception("âŒ smart_store_creator åº“æœªå¯¼å…¥ï¼Œæ— æ³•æ‰§è¡Œ V3 ç¼–ç ã€‚è¯·æ£€æŸ¥ Actions ä¾èµ–å®‰è£…ã€‚")

    # 1. ä¸´æ—¶ä¿å­˜ LightGBM æ¨¡å‹ä¸ºæ–‡æœ¬ï¼Œè¿™æ˜¯ V3 Creator çš„è¾“å…¥è¦æ±‚
    temp_lgbm_model_path = "temp_lgbm_model.txt"
    model.save_model(temp_lgbm_model_path, num_iteration=model.best_iteration)
    
    # 2. å‡†å¤‡é…ç½®æ•°æ®
    feature_to_index = {name: i for i, name in enumerate(feature_order)}
    
    # å‡†å¤‡ StandardScaler config
    std_indices = [feature_to_index[f] for f in STD_SCALER_FEATURES if f in feature_to_index]
    # åŒ¹é… scaler çš„å†…éƒ¨ç‰¹å¾é¡ºåºå’Œ full_feature_list çš„é¡ºåº
    std_data_map = {f: i for i, f in enumerate(STD_SCALER_FEATURES) if f in feature_order}
    std_mean = [std_scaler.mean_[std_data_map[f]] for f in STD_SCALER_FEATURES if f in std_data_map]
    std_scale = [std_scaler.scale_[std_data_map[f]] for f in STD_SCALER_FEATURES if f in std_data_map]

    # å‡†å¤‡ RobustScaler config
    robust_indices = [feature_to_index[f] for f in ROBUST_SCALER_FEATURES if f in feature_to_index]
    robust_data_map = {f: i for i, f in enumerate(ROBUST_SCALER_FEATURES) if f in feature_order}
    robust_center = [robust_scaler.center_[robust_data_map[f]] for f in ROBUST_SCALER_FEATURES if f in robust_data_map]
    robust_scale = [robust_scaler.scale_[robust_data_map[f]] for f in ROBUST_SCALER_FEATURES if f in robust_data_map]

    scaler_config_data: Dict[str, Any] = {
        'std': {
            'features': std_indices,
            'mean': std_mean,
            'scale': std_scale,
        },
        'robust': {
            'features': robust_indices,
            'center': robust_center,
            'scale': robust_scale,
        }
    }

    try:
        # 3. åˆå§‹åŒ–ç¼–ç å™¨
        creator = SmartStoreCreator(
            lgbm_model_path=temp_lgbm_model_path,
            feature_order=feature_order, 
            scaler_config=scaler_config_data,
            output_bin_path=model_file
        )
            
        # 4. ç¡®ä¿ç›®æ ‡ç›®å½•å­˜åœ¨
        output_dir = os.path.dirname(os.path.abspath(model_file))
        if output_dir and not os.path.exists(output_dir):
            os.makedirs(output_dir)
            
        # âš¡ æ‰§è¡Œ V3 ç¼–ç 
        creator.create_smartstore(version=3) 
        
        # 5. æ£€æŸ¥æ–‡ä»¶å¤§å°å¹¶ç¡®è®¤
        file_size = os.path.getsize(model_file)
        print(f"ğŸ‰ V3 äºŒè¿›åˆ¶æ¨¡å‹å·²æˆåŠŸä¿å­˜åˆ°: {model_file} ({file_size} å­—èŠ‚)")

    except Exception as e:
        import traceback
        traceback.print_exc()
        raise Exception(f"âŒ V3 äºŒè¿›åˆ¶æ¨¡å‹ç¼–ç å¤±è´¥: {e}")
    finally:
        # 6. æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if os.path.exists(temp_lgbm_model_path):
            os.remove(temp_lgbm_model_path)


# ==============================================================================
# 4. ä¸»ç¨‹åºæµç¨‹æ§åˆ¶
# ==============================================================================

def main() -> None:
    """
    ä¸»ç¨‹åºå…¥å£
    """
    print("=" * 60)
    print("Mihomo V3 æ™ºèƒ½æƒé‡æ¨¡å‹è®­ç»ƒ")
    print("=" * 60)
    
    if not CREATOR_AVAILABLE:
        print("è‡´å‘½é”™è¯¯: V3 ç¼–ç åº“ 'smart_store_creator' æœªå®‰è£…ã€‚ç¨‹åºç»ˆæ­¢ã€‚")
        return
    
    # æ­¥éª¤1: Go æºç è§£æ
    print("\n[æ­¥éª¤1] Go æºç è§£æ")
    try:
        parser = GoTransformParser(GO_FILE)
        feature_order = parser.get_feature_order()
        print(f"ç‰¹å¾é¡ºåºè§£æå®Œæˆï¼Œå…± {len(feature_order)} ä¸ªç‰¹å¾")
    except Exception as e:
        print(f"Go æºç è§£æå¤±è´¥: {e}")
        print("ç¨‹åºç»ˆæ­¢")
        return
    
    # æ­¥éª¤2: æ•°æ®åŠ è½½ä¸æ¸…æ´—
    print("\n[æ­¥éª¤2] æ•°æ®åŠ è½½ä¸æ¸…æ´—")
    dataset = load_and_clean_data(DATA_FILE)
    if dataset is None:
        print("æ•°æ®åŠ è½½å¤±è´¥ï¼Œç¨‹åºç»ˆæ­¢")
        return
    
    # æ­¥éª¤3: ç‰¹å¾æå–
    print("\n[æ­¥éª¤3] ç‰¹å¾æå–")
    extraction_result = extract_features_from_preprocessed(dataset, feature_order)
    if extraction_result[0] is None:
        print("ç‰¹å¾æå–å¤±è´¥ï¼Œç¨‹åºç»ˆæ­¢")
        return
    
    X, y = extraction_result
    
    # æ­¥éª¤4: ç‰¹å¾æ ‡å‡†åŒ–
    print("\n[æ­¥éª¤4] ç‰¹å¾æ ‡å‡†åŒ–")
    X_processed, std_scaler, robust_scaler = apply_feature_transforms(X, feature_order)
    
    # æ­¥éª¤5: æ•°æ®é›†åˆ’åˆ†
    print("\n[æ­¥éª¤5] è®­ç»ƒæµ‹è¯•é›†åˆ’åˆ†")
    # æ³¨æ„ï¼šè¿™é‡Œä½¿ç”¨æ ‡å‡†åŒ–åçš„æ•°æ® X_processed è¿›è¡Œåˆ’åˆ†
    X_train, X_test, y_train, y_test = train_test_split(
        X_processed, y, 
        test_size=0.2,
        random_state=42
    )
    print(f"æ•°æ®åˆ’åˆ†å®Œæˆ - è®­ç»ƒé›†: {X_train.shape}, æµ‹è¯•é›†: {X_test.shape}")
    
    # æ­¥éª¤6: æ¨¡å‹è®­ç»ƒ (è¿”å› lgb.Booster)
    print("\n[æ­¥éª¤6] æ¨¡å‹è®­ç»ƒ")
    trained_model_booster = train_lgbm_model(X_train, y_train, X_test, y_test)
    
    # æ­¥éª¤7: V3 ç¼–ç ä¿å­˜ (ä½¿ç”¨ lgb.Booster)
    print("\n[æ­¥éª¤7] æ¨¡å‹ V3 ç¼–ç ä¸ä¿å­˜")
    try:
        # å°† LightGBM çš„åŸç”Ÿ Booster å¯¹è±¡ã€Scalerã€ç‰¹å¾é¡ºåºå’Œç›®æ ‡æ–‡ä»¶è·¯å¾„ä¼ å…¥
        save_model_and_config(trained_model_booster, std_scaler, robust_scaler, feature_order, MODEL_FILE)
    except Exception as e:
        print(f"æ¨¡å‹ä¿å­˜å¤±è´¥: {e}")
        # å¦‚æœç¼–ç å¤±è´¥ï¼Œå°† Model.bin åˆ é™¤ï¼Œé˜²æ­¢ä¸Šä¼ æ— æ•ˆæ–‡ä»¶
        if os.path.exists(MODEL_FILE):
            os.remove(MODEL_FILE)
        return
        
    # è®­ç»ƒå®Œæˆæ€»ç»“
    print("\n" + "=" * 60)
    print("æ¨¡å‹è®­ç»ƒæµç¨‹å®Œæˆ")
    print(f"è¾“å‡ºæ–‡ä»¶: {MODEL_FILE}")
    print("æ¨¡å‹å¯è¿›è¡Œç”Ÿäº§ç¯å¢ƒéƒ¨ç½²")
    print("=" * 60)

# ==============================================================================
# ç¨‹åºå…¥å£ç‚¹
# ==============================================================================

if __name__ == "__main__":
    main()
